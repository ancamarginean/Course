{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives:\n",
    "\n",
    "- Load movie-review string data as  **sparse feature vectors**\n",
    "- Implement a sentiment-analysis neural network model using an **embedding** that projects data into **two dimensions**\n",
    "- **Visualize** the embedding to see what the model has learned about the relationships between words\n",
    "- Use a **pretrained** embedding GloVe (Global Vectors for word representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = keras.datasets.imdb\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The argument num_words=10000 keeps the top 10,000 most frequently occurring words in the training data. The rare words are discarded to keep the size of the data manageable.\n",
    "\n",
    "## Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training entries: {}, labels: {}\".format(len(train_data), len(train_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text of reviews have been converted to integers, where each integer represents a specific word in a dictionary. Here's what the first review looks like:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the integers back to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dictionary mapping words to an integer index\n",
    "word_index = imdb.get_word_index()\n",
    "\n",
    "# The first indices are reserved\n",
    "word_index = {k:(v+3) for k,v in word_index.items()}\n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2  # unknown\n",
    "word_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
    "word_index['comedy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_review(train_data[2]),train_labels[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data\n",
    "### Restrict the vocabulary to a smaller static vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50 informative terms that compose our model vocabulary \n",
    "informative_terms = (\"<PAD>\", \"bad\", \"great\", \"best\", \"worst\", \"fun\", \"beautiful\",\n",
    "                     \"excellent\", \"poor\", \"boring\", \"awful\", \"terrible\",\n",
    "                     \"definitely\", \"perfect\", \"liked\", \"worse\", \"waste\",\n",
    "                     \"entertaining\", \"loved\", \"unfortunately\", \"amazing\",\n",
    "                     \"enjoyed\", \"favorite\", \"horrible\", \"brilliant\", \"highly\",\n",
    "                     \"simple\", \"annoying\", \"today\", \"hilarious\", \"enjoyable\",\n",
    "                     \"dull\", \"fantastic\", \"poorly\", \"fails\", \"disappointing\",\n",
    "                     \"disappointment\", \"not\", \"him\", \"her\", \"good\", \"time\",\n",
    "                     \"?\", \".\", \"!\", \"movie\", \"film\", \"action\", \"comedy\",\n",
    "                     \"drama\", \"family\")\n",
    "\n",
    "def filtered(text):\n",
    "    return [informative_terms.index(reverse_word_index.get(i, '?')) for i in text if reverse_word_index.get(i, '?') in informative_terms ]\n",
    "def decode_filtered_review(text):\n",
    "    return ' '.join([informative_terms[i] for i in text])\n",
    "\n",
    "decode_filtered_review(filtered(train_data[2]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter all the reviews with the static vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [filtered(data) for data in train_data]\n",
    "\n",
    "test_data = [filtered(data) for data in test_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert reviews to arrays of integers of the same length\n",
    "Pad the arrays so they all have the same length, then create an integer tensor of shape max_length * num_reviews. We can use an embedding layer capable of handling this shape as the first layer in our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_data = keras.preprocessing.sequence.pad_sequences(train_data,\n",
    "                                                        value=word_index[\"<PAD>\"],\n",
    "                                                        padding='post',\n",
    "                                                        #maxlen=256) \n",
    "                                                         maxlen=50)\n",
    "\n",
    "test_data = keras.preprocessing.sequence.pad_sequences(test_data,\n",
    "                                                       value=word_index[\"<PAD>\"],\n",
    "                                                       padding='post',\n",
    "                                                       #maxlen=256)\n",
    "                                                       maxlen=50)   \n",
    "\n",
    "\n",
    "#decode_review(test_data[100]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at one of the examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_filtered_review(test_data[2]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model\n",
    "- How manu layers to use in the model\n",
    "- How many hidden units to use for each layer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab_size = 10000\n",
    "vocab_size = 51\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, 2))\n",
    "model.add(keras.layers.GlobalAveragePooling1D())\n",
    "model.add(keras.layers.Dense(16, activation=tf.nn.relu))\n",
    "model.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function and optimizer\n",
    "A model needs a loss function and an optimizer for training. Since this is a binary classification problem and the model outputs a probability (a single-unit layer with a sigmoid activation), we'll use the **binary_crossentropy** loss function.\n",
    "\n",
    "Binary_crossentropy is good for dealing with probabilities — it measures the \"distance\" between probability distributions: in our case, between the ground-truth distribution and the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create validation set\n",
    "When training, we want to check the accuracy of the model on data it hasn't seen before. Create a validation set by setting apart 10,000 examples from the original training data. (Why not use the testing set now? Our goal is to develop and tune our model using only the training data, then use the test data just once to evaluate our accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = train_data[:10000]\n",
    "partial_x_train = train_data[10000:]\n",
    "\n",
    "y_val = train_labels[:10000]\n",
    "partial_y_train = train_labels[10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "Train the model for n  epochs in mini-batches of 512 samples. This is 40 iterations over all samples in the x_train and y_train tensors. While training, monitor the model's **loss** and **accuracy** on the 10,000 samples from the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=13,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model\n",
    "And let's see how the model performs. Two values will be returned. Loss (a number which represents our error, lower values are better), and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(test_data, test_labels)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a graph of accuracy and loss over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history_dict['acc']\n",
    "val_acc = history_dict['val_acc']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()   # clear figure\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this plot, the dots represent the training loss and accuracy, and the solid lines are the validation loss and accuracy.\n",
    "\n",
    "Notice the training loss decreases with each epoch and the training accuracy increases with each epoch. This is expected when using a gradient descent optimization—it should minimize the desired quantity on every iteration.\n",
    "\n",
    "This isn't the case for the validation loss and accuracy all the time (specially for no of epochs >10) —they seem to peak after about twenty epochs. This is an example of overfitting: the model performs better on the training data than it does on data it has never seen before. After this point, the model over-optimizes and learns representations specific to the training data that do not generalize to test data.\n",
    "\n",
    "For this particular case, we could prevent overfitting by simply stopping the training after twenty or so epochs. Later, you'll see how to do this automatically with a callback.\n",
    "\n",
    "## Let's plot the 2-dimension embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.layers[0].get_weights()[0] ## takes the weights of the embedding layer\n",
    "x,y=embeddings[:,0], embeddings[:,1]\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(7,7))\n",
    "for i in range(0,vocab_size):\n",
    "    xx, yy = x[i], y[i]\n",
    "    plt.scatter(xx, yy, marker='x', color='red', alpha=0.5)\n",
    "    plt.text(xx+.001, yy+.001, informative_terms[i], fontsize=10)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, the words are distributed in the entire 2D space (in case we use an embedding of 2).\n",
    "After 2 epochs, words with negative meaning are getting closer and further away from words with positive meaning.\n",
    "- Reinitialize the model, run only one epochs and regenerate the plot. \n",
    "- Continue learning with 4 epochs.\n",
    "\n",
    "## Observe the embeddings with similarity\n",
    "Instead of plotting, you can compute similarity between two words by computing the cos of the angle between the two vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cosine similarity between two words\n",
    "embeddings = model.layers[0].get_weights()[0]\n",
    "from numpy.linalg import norm\n",
    "v1 = embeddings[informative_terms.index(\"best\")]\n",
    "v2 = embeddings[informative_terms.index(\"worst\")]\n",
    "v3 = embeddings[informative_terms.index(\"boring\")]\n",
    "\n",
    "#v1 = embedding_matrix[word_index[\"kids\"]]\n",
    "#v2 = embedding_matrix[word_index[\"school\"]]\n",
    "#v3 = embedding_matrix[word_index[\"book\"]]\n",
    "similarity1 = np.dot(v1,v2)/(norm(v1)*norm(v2))\n",
    "similarity2 = np.dot(v2,v3)/(norm(v2)*norm(v3))\n",
    "\n",
    "similarity1, similarity2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The similarity betweeb best and worst should become smaller as you train the model (almost to -1), while the similarity between worst and boring should become larger (close to 1).\n",
    "\n",
    "# Use a pretrained embedding: Glove\n",
    "We will reload the reviews data keeping 10000 words. We are using an embedding of 100 dimension from GloVe. \n",
    "Observation: from the file system, open the glove.6B.100d.txt and observe its content. It is a big file, so maybe you can inspect it with less command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "glove_data_file='glove/glove.6B.100d.txt'\n",
    "gl=pd.read_csv(glove_data_file, sep=\" \", index_col=0, header=None, quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the matrix for the initialization of the embedding layer\n",
    "The matrix has vocab_size rows, and embedding_dimension columns. \n",
    "In case of the loaded Glove model, each word has associated an embedding of 100.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "embedding_dimension=100\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dimension))\n",
    "\n",
    "for  word, i in word_index.items()   :\n",
    "    if i<vocab_size:\n",
    "        (index,)=(gl.index==word).nonzero()\n",
    "        #print(word,i, index)\n",
    "        if len(index)>0:\n",
    "            embedding_matrix[i] = gl.values[index]\n",
    "            #print(embedding_matrix[i])\n",
    "embedding_matrix[11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the pretrained embeddings as the initialization of the Embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, embedding_dimension, weights=[embedding_matrix], trainable=True))\n",
    "model.add(keras.layers.GlobalAveragePooling1D())\n",
    "model.add(keras.layers.Dense(16, activation=tf.nn.relu))\n",
    "model.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go back to the step of setting the loss and the optimizer and redo all the learning and evaluation steps with the new model. \n",
    "You can not anymore plot it in 2D space (since it is in 100D space), but you can use  [projector](https://projector.tensorflow.org/) for TensorBoard.\n",
    "\n",
    "The notebook is inspired from [TensorFlow Basic Classification Tutorial](https://www.tensorflow.org/tutorials/keras/basic_text_classification) and [Machine Learning Crash Course - Embeddings programming exercise](https://developers.google.com/machine-learning/crash-course/embeddings/programming-exercise)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
